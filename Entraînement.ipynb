{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Cy-QikFL14DpLKm5nlvN3q0P1Qln_1hy",
      "authorship_tag": "ABX9TyMMQw4IAuyWPa4MuV8xK9Y8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {}
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elidonda2-web/Alisia-7B-it-/blob/main/Entra%C3%AEnement_Alisia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Avant de commencer exÃ©cute ce code:"
      ],
      "metadata": {
        "id": "g1qyWu8WGD7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "time.sleep(60)"
      ],
      "metadata": {
        "id": "h68gkxDqGLRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# EntraÃ®nement des modÃ¨les Alisia avec format chat_template\n",
        "\n",
        "Ce notebook est utilisÃ© pour entraÃ®ner nos futurs modÃ¨les Alisia. Comme discutÃ© dans le groupe, nous allons utiliser le format **chat_template** pour nous permettre de bien Ã©voluer et de rendre le modÃ¨le multimodal.\n",
        "\n",
        "## ğŸ“ Format chat_template\n",
        "\n",
        "Le format chat_template standardise la structure des conversations. Voici un exemple :\n",
        "\n",
        "**Format :**\n",
        "```python\n",
        "{\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"Tu es Alisia, un assistant utile.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Bonjour, comment Ã§a va ?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Salut ! Je vais bien, merci. Et toi ?\"},\n",
        "        {\"role\": \"user\", \"content\": \"TrÃ¨s bien aussi !\"}\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "ğŸŒŸ MultimodalitÃ©\n",
        "\n",
        "La multimodalitÃ© signifie que le modÃ¨le peut comprendre et gÃ©nÃ©rer diffÃ©rents types de contenu : texte, images, audio. Cela permet des interactions plus riches comme dÃ©crire une image ou gÃ©nÃ©rer du contenu visuel Ã  partir d'un texte.\n",
        "\n",
        "ğŸš€ AccÃ©lÃ©ration de l'entraÃ®nement\n",
        "\n",
        "Nous utilisons Unsloth pour rendre l'entraÃ®nement 2x plus rapide avec moins de mÃ©moire utilisÃ©e.\n",
        "\n",
        "ğŸ“š BibliothÃ¨ques Ã  importer\n",
        "\n",
        "Les bibliothÃ¨ques principales Ã  importer sont :\n",
        "\n",
        "Â· unsloth pour l'accÃ©lÃ©ration\n",
        "Â· transformers pour les modÃ¨les et tokenizers\n",
        "Â· datasets pour la gestion des donnÃ©es\n",
        "Â· accelerate pour l'entraÃ®nement distribuÃ©\n",
        "\n",
        "# voici le code:"
      ],
      "metadata": {
        "id": "MWy_ujtjFz2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "id": "PwsmKVtj41Cj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ¯ Notre stratÃ©gie pour crÃ©er Alisia\n",
        "\n",
        "### ğŸ—ï¸ Une approche intelligente et efficace\n",
        "\n",
        "PlutÃ´t que de tout construire depuis zÃ©ro (ce qui prendrait des mois et coÃ»terait trÃ¨s cher), nous utilisons une mÃ©thode Ã©prouvÃ©e :\n",
        "\n",
        "**Qwen2-7B est notre fondation de dÃ©part**\n",
        "- C'est comme si on reprenait un cerveau dÃ©jÃ  Ã©duquÃ©\n",
        "- Il connaÃ®t dÃ©jÃ  le franÃ§ais, l'anglais, la logique, le raisonnement\n",
        "- On Ã©vite de rÃ©inventer la roue\n",
        "\n",
        "### ğŸ¨ Notre vraie valeur : personnaliser Alisia\n",
        "\n",
        "**Ce qui va rendre Alisia unique, c'est NOTRE travail :**\n",
        "- âœ… On va lui donner sa personnalitÃ© propre\n",
        "- âœ… On va lui apprendre notre faÃ§on de communiquer\n",
        "- âœ… On va l'entraÃ®ner sur nos sujets prÃ©fÃ©rÃ©s\n",
        "- âœ… On va modeler ses rÃ©ponses selon notre style\n",
        "\n",
        "### ğŸš€ RÃ©sultat final\n",
        "Au bout du processus, Qwen2-7B ne sera plus reconnaissable. Vous parlerez avec **Alisia** - une intelligence unique qui portera notre empreinte.\n",
        "\n",
        "**L'avantage** : On obtient rapidement un assistant performant, tout en gardant 100% de notre identitÃ©.\n",
        "\n",
        "C'est la mÃ©thode utilisÃ©e par la majoritÃ© des projets d'IA aujourd'hui : partir d'une base solide pour construire quelque chose d'unique plus rapidement !\n",
        "\n",
        "# voici le code:"
      ],
      "metadata": {
        "id": "43xB1dQD8yyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "# Try more models at https://huggingface.co/unsloth!\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Unsloth/Qwen2-7B\", # Reminder we support ANY Hugging Face model!\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "bIJtzudE94MQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ğŸ”§ Explication du code pour dÃ©butants\n",
        "\n",
        "## ğŸ¯ Ce que fait cette partie :\n",
        "\n",
        "Cette fonction prÃ©pare notre modÃ¨le pour l'entraÃ®nement en utilisant une technique spÃ©ciale qui est **Ã©conome et efficace**.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“– Explication ligne par ligne :\n",
        "\n",
        "### **La base :**\n",
        "```python\n",
        "model = FastLanguageModel.get_peft_model(model, ...)\n",
        "```\n",
        "\n",
        "â†’ \"Prends notre modÃ¨le Qwen2-7B et prÃ©pare-le pour un entraÃ®nement optimisÃ©\"\n",
        "\n",
        "Les paramÃ¨tres importants :\n",
        "\n",
        "r = 16\n",
        "\n",
        "Â· C'est le \"niveau de personnalisation\"\n",
        "Â· Comme si on disait : \"On va modifier 16 aspects du modÃ¨le pour lui donner la personnalitÃ© d'Alisia\"\n",
        "Â· Plus ce nombre est grand, plus la personnalisation est forte\n",
        "\n",
        "target_modules = [\"q_proj\", \"k_proj\", ...]\n",
        "\n",
        "Â· Ce sont les \"parties du cerveau\" qu'on va adapter\n",
        "Â· Chaque module est comme une zone spÃ©cialisÃ©e :\n",
        "  Â· q_proj, k_proj, v_proj = la comprÃ©hension du langage\n",
        "  Â· gate_proj, up_proj, down_proj = la gÃ©nÃ©ration des rÃ©ponses\n",
        "Â· On cible les zones les plus importantes pour la conversation\n",
        "\n",
        "lora_alpha = 16\n",
        "\n",
        "Â· C'est \"l'intensitÃ©\" des modifications\n",
        "Â· Comme le volume de nos ajustements\n",
        "\n",
        "lora_dropout = 0\n",
        "\n",
        "Â· DÃ©sactive une fonction optionnelle pour plus de stabilitÃ©\n",
        "\n",
        "bias = \"none\"\n",
        "\n",
        "Â· On garde les rÃ©glages originaux du modÃ¨le (plus simple)\n",
        "\n",
        "use_gradient_checkpointing = \"unsloth\"\n",
        "\n",
        "Â· ğŸš€ La magie d'Unsloth !\n",
        "Â· RÃ©duit la mÃ©moire utilisÃ©e de 30%\n",
        "Â· Permet de traiter 2x plus de donnÃ©es Ã  la fois\n",
        "Â· = EntraÃ®nement plus rapide et moins cher\n",
        "\n",
        "---\n",
        "\n",
        "ğŸ’¡ En rÃ©sumÃ© :\n",
        "\n",
        "Cette technique (appelÃ©e LoRA) nous permet de :\n",
        "\n",
        "Â· âœ… Personnaliser le modÃ¨le pour crÃ©er Alisia\n",
        "Â· âœ… Ã‰conomiser Ã©normÃ©ment de mÃ©moire et de temps\n",
        "Â· âœ… Garder toutes les connaissances de base de Qwen2-7B\n",
        "Â· âœ… Ajouter seulement notre touche personnelle\n",
        "\n",
        "C'est comme si on ajoutait une \"couche de personnalitÃ©\" par-dessus l'intelligence existante, sans tout rÃ©Ã©crire !\n",
        "\n",
        "RÃ©sultat : On obtient Alisia plus rapidement, avec moins de ressources ! ğŸ‰\n",
        "\n",
        "# voici le code:"
      ],
      "metadata": {
        "id": "z7IYwi_b--c6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32, # âœ… AugmentÃ© Ã  32 pour plus de personnalisation mais on peut toujours revenir Ã  16. Je sais pourquoi.\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32, # âœ… AjustÃ© en consÃ©quence\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "gv3fxWBJ_ljq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ğŸ”§ Explication du code : PrÃ©paration des donnÃ©es\n",
        "\n",
        "## ğŸ¯ Ce que fait cette partie :\n",
        "\n",
        "Cette section prÃ©pare nos conversations pour que le modÃ¨le puisse les comprendre et apprendre d'elles.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“– Explication ligne par ligne :\n",
        "\n",
        "### **Chargement des donnÃ©es**\n",
        "```python\n",
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
        "```\n",
        "\n",
        "â†’ \"TÃ©lÃ©charge 100,000 conversations d'entraÃ®nement depuis internet\"\n",
        "\n",
        "Standardisation (Nettoyage et unification)\n",
        "\n",
        "```python\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "```\n",
        "\n",
        "â†’ Pourquoi on fait Ã§a ?\n",
        "Ce dataset particulier (FineTome-100k) contient des conversations qui viennent de diffÃ©rentes sources et qui peuvent avoir des formats variÃ©s.\n",
        "La standardisation les transforme toutes dans le mÃªme format standard, comme si on prenait des livres Ã©crits avec diffÃ©rentes polices et marges, et qu'on les rÃ©imprimait tous de la mÃªme faÃ§on.\n",
        "\n",
        "Sans standardisation : âŒ\n",
        "\n",
        "Â· Certaines conversations seraient illisibles pour le modÃ¨le\n",
        "Â· Le modÃ¨le ne saurait pas oÃ¹ trouver les questions et rÃ©ponses\n",
        "Â· L'entraÃ®nement serait dÃ©sordonnÃ©\n",
        "\n",
        "#Avec standardisation : âœ…\n",
        "\n",
        "Â· Toutes les conversations ont la mÃªme structure\n",
        "Â· Le modÃ¨le comprend facilement qui parle et quand\n",
        "Â· L'apprentissage est plus efficace\n",
        "\n",
        "Le format de conversation (Chat Template)\n",
        "\n",
        "```python\n",
        "tokenizer.chat_template = \"\"\"...\"\"\"\n",
        "```\n",
        "\n",
        "â†’ \"On dÃ©finit comment Alisia doit se prÃ©senter et comment structurer les conversations\"\n",
        "\n",
        "Exemple de ce que Ã§a produit :\n",
        "\n",
        "```\n",
        "<|im_start|>system\n",
        "You are Alisia, a helpful assistant...<|im_end|>\n",
        "<|im_start|>user\n",
        "Bonjour !<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Salut ! Comment puis-je t'aider ?<|im_end|>\n",
        "```\n",
        "\n",
        "La fonction de formatage\n",
        "\n",
        "```python\n",
        "def formatting_prompts_func(examples):\n",
        "```\n",
        "\n",
        "â†’ \"Prend chaque conversation et la transforme dans le format qu'Alisia comprend\"\n",
        "\n",
        "Application du formatage\n",
        "\n",
        "```python\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "```\n",
        "\n",
        "â†’ \"On applique la transformation Ã  TOUTES les 100,000 conversations, par lots pour aller plus vite\"\n",
        "\n",
        "---\n",
        "\n",
        "#Important Ã  comprendre :\n",
        "\n",
        "La standardisation peut varier selon le dataset :\n",
        "\n",
        "#Pourquoi Ã§a change :\n",
        "\n",
        "Â· Chaque dataset a ses particularitÃ©s\n",
        "Â· Certains datasets sont dÃ©jÃ  bien organisÃ©s, d'autres non\n",
        "Â· On adapte le nettoyage selon les besoins\n",
        "\n",
        "#Ce qui reste constant :\n",
        "\n",
        "Â· Le besoin d'avoir des donnÃ©es bien organisÃ©es\n",
        "Â· L'importance d'un format cohÃ©rent que nous utiliserons\n",
        "Â· La prÃ©paration pour un bon apprentissage\n",
        "\n",
        "---\n",
        "\n",
        "#En rÃ©sumÃ© :\n",
        "\n",
        "On standardise parce que ce dataset en a besoin pour Ãªtre utilisable. C'est comme ranger et nettoyer des ingrÃ©dients avant de cuisiner : essentiel pour un bon rÃ©sultat ! ğŸ³\n",
        "\n",
        "#voici le code:"
      ],
      "metadata": {
        "id": "n2geWH23Lksk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ğŸ“˜ PrÃ©paration du Dataset et du Chat Template (avec message systÃ¨me) ---\n",
        "\n",
        "from datasets import load_dataset\n",
        "from unsloth import standardize_sharegpt\n",
        "\n",
        "# 1ï¸âƒ£ Charger le dataset\n",
        "print(\"ğŸ”¹ Chargement du dataset...\")\n",
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
        "\n",
        "# 2ï¸âƒ£ Standardiser le format pour obtenir 'conversations'\n",
        "print(\"ğŸ”¹ Standardisation du dataset...\")\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "\n",
        "# 3ï¸âƒ£ DÃ©finir ton propre Chat Template (ChatML + message systÃ¨me par dÃ©faut)\n",
        "tokenizer.chat_template = \"\"\"<|im_start|>system\n",
        "You are Alisia, a helpful, precise, and knowledgeable assistant created by the Alisia Research Team.<|im_end|>\n",
        "{% for message in messages %}\n",
        "<|im_start|>{{ message['role'] }}\n",
        "{{ message['content'] }}<|im_end|>\n",
        "{% endfor %}\n",
        "{% if add_generation_prompt %}<|im_start|>assistant\n",
        "{% endif %}\"\"\"\n",
        "\n",
        "\n",
        "# 4ï¸âƒ£ Fonction pour formater les conversations\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [\n",
        "        tokenizer.apply_chat_template(\n",
        "            convo,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,  # Ne pas ajouter <|im_start|>assistant Ã  la fin\n",
        "        )\n",
        "        for convo in convos\n",
        "    ]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# 5ï¸âƒ£ Appliquer le formatage sur tout le dataset\n",
        "print(\"ğŸ”¹ Application du formatage des conversations...\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# âœ… VÃ©rification rapide d'un exemple\n",
        "print(\"âœ… Exemple de texte formatÃ© :\\n\")\n",
        "print(dataset[5][\"text\"])"
      ],
      "metadata": {
        "id": "vYUhYLY7HkOg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ğŸ‹ï¸ Explication du code : Configuration de l'entraÃ®nement\n",
        "\n",
        "## ğŸ¯ Ce que fait cette partie :\n",
        "\n",
        "Cette section configure et lance l'entraÃ®nement d'Alisia avec nos donnÃ©es prÃ©parÃ©es.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“– Explication simple :\n",
        "\n",
        "```python\n",
        "trainer = SFTTrainer(...)\n",
        "```\n",
        "\n",
        "â†’ \"On crÃ©e un coach intelligent qui va entraÃ®ner Alisia\"\n",
        "\n",
        "Les paramÃ¨tres principaux :\n",
        "\n",
        "Â· model = model â†’ Le modÃ¨le Qwen2-7B qu'on va personnaliser\n",
        "Â· tokenizer = tokenizer â†’ Le traducteur qui comprend nos conversations\n",
        "Â· train_dataset = dataset â†’ Nos 100,000 conversations prÃ©parÃ©es\n",
        "Â· max_steps = 60 â†’ On fait 60 cycles d'entraÃ®nement (pas trop pour commencer)\n",
        "Â· learning_rate = 2e-4 â†’ La vitesse d'apprentissage (ni trop lent, ni trop rapide)\n",
        "Â· per_device_train_batch_size = 2 â†’ 2 conversations traitÃ©es en mÃªme temps\n",
        "\n",
        "---\n",
        "\n",
        "#En rÃ©sumÃ© :\n",
        "\n",
        "Ce code lance l'entraÃ®nement oÃ¹ Alisia apprend Ã  partir de nos conversations. Le coach ajuste progressivement sa personnalitÃ© pour qu'elle rÃ©ponde comme on le souhaite !\n",
        "\n",
        "Prochaine Ã©tape : Une fois cet entraÃ®nement terminÃ©, Alisia aura sa propre personnalitÃ© ! ğŸš€\n",
        "\n",
        "##voici le code:"
      ],
      "metadata": {
        "id": "TmSbFo-P0Pmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "26V38Qco1DOj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##on peut lancer l'entraÃ®nement et sa peut prendre des minutes selon:\n",
        "\n",
        "**la connexion:** vÃ©rifier sans arrÃªt votre connexion mobile.\n",
        "\n",
        "\n",
        "**la taille de datase:** plus le dataset est Ã©levÃ©, plus vous devrez patienter.\n",
        "\n",
        "##Voici le code de lancement:"
      ],
      "metadata": {
        "id": "1V9_FG6V1KpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "pBEZKBLe2SlS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##â™¡ Mieux vaut prÃ©venir que guÃ©rir on doit envoyer le fichier dans drive tout d'abord on va enregistrer le fichier qui contient le nouveau poids de modÃ¨le."
      ],
      "metadata": {
        "id": "KoxPwrOX4t_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "z56B3b3N5x7u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ensuite:"
      ],
      "metadata": {
        "id": "8vJEve8y58-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# assure toi que ton drive qu'on tient >5G de stockage\n",
        "\n",
        "# ğŸ—» Sauvegarde vers Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Monter Google Drive\n",
        "print(\"ğŸ“ Montage de Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Chemin du modÃ¨le LoRA et destination\n",
        "lora_model_path = \"/content/lora_model\"  # Chemin actuel\n",
        "drive_backup_path = \"/content/drive/MyDrive/lora_model\"  # Chemin dans Drive\n",
        "\n",
        "# VÃ©rifier si le modÃ¨le LoRA existe\n",
        "if os.path.exists(lora_model_path):\n",
        "    print(f\"âœ… ModÃ¨le LoRA trouvÃ© dans : {lora_model_path}\")\n",
        "\n",
        "    # CrÃ©er le dossier de sauvegarde dans Drive\n",
        "    os.makedirs(drive_backup_path, exist_ok=True)\n",
        "\n",
        "    # Copier tout le dossier vers Drive\n",
        "    print(\"ğŸ“¤ Copie du modÃ¨le vers Google Drive...\")\n",
        "    shutil.copytree(lora_model_path, drive_backup_path, dirs_exist_ok=True)\n",
        "\n",
        "    print(\"ğŸ‰ **SAUVEGARDE RÃ‰USSIE !**\")\n",
        "    print(f\"ğŸ“‚ ModÃ¨le sauvegardÃ© dans : {drive_backup_path}\")\n",
        "    print(f\"ğŸ“Š Taille du dossier : {sum(os.path.getsize(os.path.join(dirpath, filename)) for dirpath, dirnames, filenames in os.walk(lora_model_path) for filename in filenames) / (1024*1024):.2f} MB\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ ERREUR : Dossier lora_model non trouvÃ© !\")\n",
        "    print(\"ğŸ’¡ VÃ©rifie que l'entraÃ®nement s'est bien terminÃ©\")\n",
        "\n",
        "# VÃ©rification finale\n",
        "if os.path.exists(drive_backup_path):\n",
        "    print(\"\\nğŸ” **VÃ‰RIFICATION :**\")\n",
        "    files = os.listdir(drive_backup_path)\n",
        "    print(f\"ğŸ“„ Fichiers sauvegardÃ©s : {len(files)}\")\n",
        "    for file in files:\n",
        "        print(f\"   - {file}\")\n",
        "    print(\"\\nâœ… **ModÃ¨le Alisia en sÃ©curitÃ© dans Google Drive !**\")"
      ],
      "metadata": {
        "id": "NjzMr9rA6Ccl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Si vous avez encore de la mÃ©moire vous pouvez tester si sa fonctionne:"
      ],
      "metadata": {
        "id": "VLm0FRuV97YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Quel est la capitale de la France.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1208,\n",
        "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
      ],
      "metadata": {
        "id": "Bc1ZUw9E-ODP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NB: sa permet juste d'avoir les adapters LoRA pas le modÃ¨le complÃ¨te faite de recherche sur internet et pour comprendre c'est quoi LoRA."
      ],
      "metadata": {
        "id": "93y1SJy6_J5y"
      }
    }
  ]
}
