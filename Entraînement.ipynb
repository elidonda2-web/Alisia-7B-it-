{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Cy-QikFL14DpLKm5nlvN3q0P1Qln_1hy",
      "authorship_tag": "ABX9TyMMQw4IAuyWPa4MuV8xK9Y8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {}
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elidonda2-web/Alisia-7B-it-/blob/main/Entra%C3%AEnement_Alisia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Avant de commencer exécute ce code:"
      ],
      "metadata": {
        "id": "g1qyWu8WGD7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "time.sleep(60)"
      ],
      "metadata": {
        "id": "h68gkxDqGLRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Entraînement des modèles Alisia avec format chat_template\n",
        "\n",
        "Ce notebook est utilisé pour entraîner nos futurs modèles Alisia. Comme discuté dans le groupe, nous allons utiliser le format **chat_template** pour nous permettre de bien évoluer et de rendre le modèle multimodal.\n",
        "\n",
        "## 📝 Format chat_template\n",
        "\n",
        "Le format chat_template standardise la structure des conversations. Voici un exemple :\n",
        "\n",
        "**Format :**\n",
        "```python\n",
        "{\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"Tu es Alisia, un assistant utile.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Bonjour, comment ça va ?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Salut ! Je vais bien, merci. Et toi ?\"},\n",
        "        {\"role\": \"user\", \"content\": \"Très bien aussi !\"}\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "🌟 Multimodalité\n",
        "\n",
        "La multimodalité signifie que le modèle peut comprendre et générer différents types de contenu : texte, images, audio. Cela permet des interactions plus riches comme décrire une image ou générer du contenu visuel à partir d'un texte.\n",
        "\n",
        "🚀 Accélération de l'entraînement\n",
        "\n",
        "Nous utilisons Unsloth pour rendre l'entraînement 2x plus rapide avec moins de mémoire utilisée.\n",
        "\n",
        "📚 Bibliothèques à importer\n",
        "\n",
        "Les bibliothèques principales à importer sont :\n",
        "\n",
        "· unsloth pour l'accélération\n",
        "· transformers pour les modèles et tokenizers\n",
        "· datasets pour la gestion des données\n",
        "· accelerate pour l'entraînement distribué\n",
        "\n",
        "# voici le code:"
      ],
      "metadata": {
        "id": "MWy_ujtjFz2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "id": "PwsmKVtj41Cj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🎯 Notre stratégie pour créer Alisia\n",
        "\n",
        "### 🏗️ Une approche intelligente et efficace\n",
        "\n",
        "Plutôt que de tout construire depuis zéro (ce qui prendrait des mois et coûterait très cher), nous utilisons une méthode éprouvée :\n",
        "\n",
        "**Qwen2-7B est notre fondation de départ**\n",
        "- C'est comme si on reprenait un cerveau déjà éduqué\n",
        "- Il connaît déjà le français, l'anglais, la logique, le raisonnement\n",
        "- On évite de réinventer la roue\n",
        "\n",
        "### 🎨 Notre vraie valeur : personnaliser Alisia\n",
        "\n",
        "**Ce qui va rendre Alisia unique, c'est NOTRE travail :**\n",
        "- ✅ On va lui donner sa personnalité propre\n",
        "- ✅ On va lui apprendre notre façon de communiquer\n",
        "- ✅ On va l'entraîner sur nos sujets préférés\n",
        "- ✅ On va modeler ses réponses selon notre style\n",
        "\n",
        "### 🚀 Résultat final\n",
        "Au bout du processus, Qwen2-7B ne sera plus reconnaissable. Vous parlerez avec **Alisia** - une intelligence unique qui portera notre empreinte.\n",
        "\n",
        "**L'avantage** : On obtient rapidement un assistant performant, tout en gardant 100% de notre identité.\n",
        "\n",
        "C'est la méthode utilisée par la majorité des projets d'IA aujourd'hui : partir d'une base solide pour construire quelque chose d'unique plus rapidement !\n",
        "\n",
        "# voici le code:"
      ],
      "metadata": {
        "id": "43xB1dQD8yyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "# Try more models at https://huggingface.co/unsloth!\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Unsloth/Qwen2-7B\", # Reminder we support ANY Hugging Face model!\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "bIJtzudE94MQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 🔧 Explication du code pour débutants\n",
        "\n",
        "## 🎯 Ce que fait cette partie :\n",
        "\n",
        "Cette fonction prépare notre modèle pour l'entraînement en utilisant une technique spéciale qui est **économe et efficace**.\n",
        "\n",
        "---\n",
        "\n",
        "## 📖 Explication ligne par ligne :\n",
        "\n",
        "### **La base :**\n",
        "```python\n",
        "model = FastLanguageModel.get_peft_model(model, ...)\n",
        "```\n",
        "\n",
        "→ \"Prends notre modèle Qwen2-7B et prépare-le pour un entraînement optimisé\"\n",
        "\n",
        "Les paramètres importants :\n",
        "\n",
        "r = 16\n",
        "\n",
        "· C'est le \"niveau de personnalisation\"\n",
        "· Comme si on disait : \"On va modifier 16 aspects du modèle pour lui donner la personnalité d'Alisia\"\n",
        "· Plus ce nombre est grand, plus la personnalisation est forte\n",
        "\n",
        "target_modules = [\"q_proj\", \"k_proj\", ...]\n",
        "\n",
        "· Ce sont les \"parties du cerveau\" qu'on va adapter\n",
        "· Chaque module est comme une zone spécialisée :\n",
        "  · q_proj, k_proj, v_proj = la compréhension du langage\n",
        "  · gate_proj, up_proj, down_proj = la génération des réponses\n",
        "· On cible les zones les plus importantes pour la conversation\n",
        "\n",
        "lora_alpha = 16\n",
        "\n",
        "· C'est \"l'intensité\" des modifications\n",
        "· Comme le volume de nos ajustements\n",
        "\n",
        "lora_dropout = 0\n",
        "\n",
        "· Désactive une fonction optionnelle pour plus de stabilité\n",
        "\n",
        "bias = \"none\"\n",
        "\n",
        "· On garde les réglages originaux du modèle (plus simple)\n",
        "\n",
        "use_gradient_checkpointing = \"unsloth\"\n",
        "\n",
        "· 🚀 La magie d'Unsloth !\n",
        "· Réduit la mémoire utilisée de 30%\n",
        "· Permet de traiter 2x plus de données à la fois\n",
        "· = Entraînement plus rapide et moins cher\n",
        "\n",
        "---\n",
        "\n",
        "💡 En résumé :\n",
        "\n",
        "Cette technique (appelée LoRA) nous permet de :\n",
        "\n",
        "· ✅ Personnaliser le modèle pour créer Alisia\n",
        "· ✅ Économiser énormément de mémoire et de temps\n",
        "· ✅ Garder toutes les connaissances de base de Qwen2-7B\n",
        "· ✅ Ajouter seulement notre touche personnelle\n",
        "\n",
        "C'est comme si on ajoutait une \"couche de personnalité\" par-dessus l'intelligence existante, sans tout réécrire !\n",
        "\n",
        "Résultat : On obtient Alisia plus rapidement, avec moins de ressources ! 🎉\n",
        "\n",
        "# voici le code:"
      ],
      "metadata": {
        "id": "z7IYwi_b--c6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32, # ✅ Augmenté à 32 pour plus de personnalisation mais on peut toujours revenir à 16. Je sais pourquoi.\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32, # ✅ Ajusté en conséquence\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "gv3fxWBJ_ljq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 🔧 Explication du code : Préparation des données\n",
        "\n",
        "## 🎯 Ce que fait cette partie :\n",
        "\n",
        "Cette section prépare nos conversations pour que le modèle puisse les comprendre et apprendre d'elles.\n",
        "\n",
        "---\n",
        "\n",
        "## 📖 Explication ligne par ligne :\n",
        "\n",
        "### **Chargement des données**\n",
        "```python\n",
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
        "```\n",
        "\n",
        "→ \"Télécharge 100,000 conversations d'entraînement depuis internet\"\n",
        "\n",
        "Standardisation (Nettoyage et unification)\n",
        "\n",
        "```python\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "```\n",
        "\n",
        "→ Pourquoi on fait ça ?\n",
        "Ce dataset particulier (FineTome-100k) contient des conversations qui viennent de différentes sources et qui peuvent avoir des formats variés.\n",
        "La standardisation les transforme toutes dans le même format standard, comme si on prenait des livres écrits avec différentes polices et marges, et qu'on les réimprimait tous de la même façon.\n",
        "\n",
        "Sans standardisation : ❌\n",
        "\n",
        "· Certaines conversations seraient illisibles pour le modèle\n",
        "· Le modèle ne saurait pas où trouver les questions et réponses\n",
        "· L'entraînement serait désordonné\n",
        "\n",
        "#Avec standardisation : ✅\n",
        "\n",
        "· Toutes les conversations ont la même structure\n",
        "· Le modèle comprend facilement qui parle et quand\n",
        "· L'apprentissage est plus efficace\n",
        "\n",
        "Le format de conversation (Chat Template)\n",
        "\n",
        "```python\n",
        "tokenizer.chat_template = \"\"\"...\"\"\"\n",
        "```\n",
        "\n",
        "→ \"On définit comment Alisia doit se présenter et comment structurer les conversations\"\n",
        "\n",
        "Exemple de ce que ça produit :\n",
        "\n",
        "```\n",
        "<|im_start|>system\n",
        "You are Alisia, a helpful assistant...<|im_end|>\n",
        "<|im_start|>user\n",
        "Bonjour !<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Salut ! Comment puis-je t'aider ?<|im_end|>\n",
        "```\n",
        "\n",
        "La fonction de formatage\n",
        "\n",
        "```python\n",
        "def formatting_prompts_func(examples):\n",
        "```\n",
        "\n",
        "→ \"Prend chaque conversation et la transforme dans le format qu'Alisia comprend\"\n",
        "\n",
        "Application du formatage\n",
        "\n",
        "```python\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "```\n",
        "\n",
        "→ \"On applique la transformation à TOUTES les 100,000 conversations, par lots pour aller plus vite\"\n",
        "\n",
        "---\n",
        "\n",
        "#Important à comprendre :\n",
        "\n",
        "La standardisation peut varier selon le dataset :\n",
        "\n",
        "#Pourquoi ça change :\n",
        "\n",
        "· Chaque dataset a ses particularités\n",
        "· Certains datasets sont déjà bien organisés, d'autres non\n",
        "· On adapte le nettoyage selon les besoins\n",
        "\n",
        "#Ce qui reste constant :\n",
        "\n",
        "· Le besoin d'avoir des données bien organisées\n",
        "· L'importance d'un format cohérent que nous utiliserons\n",
        "· La préparation pour un bon apprentissage\n",
        "\n",
        "---\n",
        "\n",
        "#En résumé :\n",
        "\n",
        "On standardise parce que ce dataset en a besoin pour être utilisable. C'est comme ranger et nettoyer des ingrédients avant de cuisiner : essentiel pour un bon résultat ! 🍳\n",
        "\n",
        "#voici le code:"
      ],
      "metadata": {
        "id": "n2geWH23Lksk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 📘 Préparation du Dataset et du Chat Template (avec message système) ---\n",
        "\n",
        "from datasets import load_dataset\n",
        "from unsloth import standardize_sharegpt\n",
        "\n",
        "# 1️⃣ Charger le dataset\n",
        "print(\"🔹 Chargement du dataset...\")\n",
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
        "\n",
        "# 2️⃣ Standardiser le format pour obtenir 'conversations'\n",
        "print(\"🔹 Standardisation du dataset...\")\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "\n",
        "# 3️⃣ Définir ton propre Chat Template (ChatML + message système par défaut)\n",
        "tokenizer.chat_template = \"\"\"<|im_start|>system\n",
        "You are Alisia, a helpful, precise, and knowledgeable assistant created by the Alisia Research Team.<|im_end|>\n",
        "{% for message in messages %}\n",
        "<|im_start|>{{ message['role'] }}\n",
        "{{ message['content'] }}<|im_end|>\n",
        "{% endfor %}\n",
        "{% if add_generation_prompt %}<|im_start|>assistant\n",
        "{% endif %}\"\"\"\n",
        "\n",
        "\n",
        "# 4️⃣ Fonction pour formater les conversations\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [\n",
        "        tokenizer.apply_chat_template(\n",
        "            convo,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,  # Ne pas ajouter <|im_start|>assistant à la fin\n",
        "        )\n",
        "        for convo in convos\n",
        "    ]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# 5️⃣ Appliquer le formatage sur tout le dataset\n",
        "print(\"🔹 Application du formatage des conversations...\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# ✅ Vérification rapide d'un exemple\n",
        "print(\"✅ Exemple de texte formaté :\\n\")\n",
        "print(dataset[5][\"text\"])"
      ],
      "metadata": {
        "id": "vYUhYLY7HkOg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 🏋️ Explication du code : Configuration de l'entraînement\n",
        "\n",
        "## 🎯 Ce que fait cette partie :\n",
        "\n",
        "Cette section configure et lance l'entraînement d'Alisia avec nos données préparées.\n",
        "\n",
        "---\n",
        "\n",
        "## 📖 Explication simple :\n",
        "\n",
        "```python\n",
        "trainer = SFTTrainer(...)\n",
        "```\n",
        "\n",
        "→ \"On crée un coach intelligent qui va entraîner Alisia\"\n",
        "\n",
        "Les paramètres principaux :\n",
        "\n",
        "· model = model → Le modèle Qwen2-7B qu'on va personnaliser\n",
        "· tokenizer = tokenizer → Le traducteur qui comprend nos conversations\n",
        "· train_dataset = dataset → Nos 100,000 conversations préparées\n",
        "· max_steps = 60 → On fait 60 cycles d'entraînement (pas trop pour commencer)\n",
        "· learning_rate = 2e-4 → La vitesse d'apprentissage (ni trop lent, ni trop rapide)\n",
        "· per_device_train_batch_size = 2 → 2 conversations traitées en même temps\n",
        "\n",
        "---\n",
        "\n",
        "#En résumé :\n",
        "\n",
        "Ce code lance l'entraînement où Alisia apprend à partir de nos conversations. Le coach ajuste progressivement sa personnalité pour qu'elle réponde comme on le souhaite !\n",
        "\n",
        "Prochaine étape : Une fois cet entraînement terminé, Alisia aura sa propre personnalité ! 🚀\n",
        "\n",
        "##voici le code:"
      ],
      "metadata": {
        "id": "TmSbFo-P0Pmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "26V38Qco1DOj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##on peut lancer l'entraînement et sa peut prendre des minutes selon:\n",
        "\n",
        "**la connexion:** vérifier sans arrêt votre connexion mobile.\n",
        "\n",
        "\n",
        "**la taille de datase:** plus le dataset est élevé, plus vous devrez patienter.\n",
        "\n",
        "##Voici le code de lancement:"
      ],
      "metadata": {
        "id": "1V9_FG6V1KpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "pBEZKBLe2SlS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##♡ Mieux vaut prévenir que guérir on doit envoyer le fichier dans drive tout d'abord on va enregistrer le fichier qui contient le nouveau poids de modèle."
      ],
      "metadata": {
        "id": "KoxPwrOX4t_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "z56B3b3N5x7u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ensuite:"
      ],
      "metadata": {
        "id": "8vJEve8y58-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# assure toi que ton drive qu'on tient >5G de stockage\n",
        "\n",
        "# 🗻 Sauvegarde vers Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Monter Google Drive\n",
        "print(\"📁 Montage de Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Chemin du modèle LoRA et destination\n",
        "lora_model_path = \"/content/lora_model\"  # Chemin actuel\n",
        "drive_backup_path = \"/content/drive/MyDrive/lora_model\"  # Chemin dans Drive\n",
        "\n",
        "# Vérifier si le modèle LoRA existe\n",
        "if os.path.exists(lora_model_path):\n",
        "    print(f\"✅ Modèle LoRA trouvé dans : {lora_model_path}\")\n",
        "\n",
        "    # Créer le dossier de sauvegarde dans Drive\n",
        "    os.makedirs(drive_backup_path, exist_ok=True)\n",
        "\n",
        "    # Copier tout le dossier vers Drive\n",
        "    print(\"📤 Copie du modèle vers Google Drive...\")\n",
        "    shutil.copytree(lora_model_path, drive_backup_path, dirs_exist_ok=True)\n",
        "\n",
        "    print(\"🎉 **SAUVEGARDE RÉUSSIE !**\")\n",
        "    print(f\"📂 Modèle sauvegardé dans : {drive_backup_path}\")\n",
        "    print(f\"📊 Taille du dossier : {sum(os.path.getsize(os.path.join(dirpath, filename)) for dirpath, dirnames, filenames in os.walk(lora_model_path) for filename in filenames) / (1024*1024):.2f} MB\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ ERREUR : Dossier lora_model non trouvé !\")\n",
        "    print(\"💡 Vérifie que l'entraînement s'est bien terminé\")\n",
        "\n",
        "# Vérification finale\n",
        "if os.path.exists(drive_backup_path):\n",
        "    print(\"\\n🔍 **VÉRIFICATION :**\")\n",
        "    files = os.listdir(drive_backup_path)\n",
        "    print(f\"📄 Fichiers sauvegardés : {len(files)}\")\n",
        "    for file in files:\n",
        "        print(f\"   - {file}\")\n",
        "    print(\"\\n✅ **Modèle Alisia en sécurité dans Google Drive !**\")"
      ],
      "metadata": {
        "id": "NjzMr9rA6Ccl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Si vous avez encore de la mémoire vous pouvez tester si sa fonctionne:"
      ],
      "metadata": {
        "id": "VLm0FRuV97YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Quel est la capitale de la France.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1208,\n",
        "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
      ],
      "metadata": {
        "id": "Bc1ZUw9E-ODP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NB: sa permet juste d'avoir les adapters LoRA pas le modèle complète faite de recherche sur internet et pour comprendre c'est quoi LoRA."
      ],
      "metadata": {
        "id": "93y1SJy6_J5y"
      }
    }
  ]
}
