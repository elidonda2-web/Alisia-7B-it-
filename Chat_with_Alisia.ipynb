{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfa/sv3eCq137aFJIt0+ZH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elidonda2-web/Alisia-7B-it-/blob/main/Chat_with_Alisia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bv0ivPiwndG-"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio\n",
        "!pip install -q llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Votre_id=      #Tu dois coller devant le signe d'√©galisation lien que tu as copier depuis drive."
      ],
      "metadata": {
        "id": "FS75BDNonzdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown --quit\n",
        "!gdown Votre_id -O /kaggle/working/Alisia.Q4_K_M.gguf"
      ],
      "metadata": {
        "id": "2EFksJFAoG0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **********************************************************************\n",
        "# Alisis - Chat UI complete (GGUF + Streaming)\n",
        "# **********************************************************************\n",
        "from llama_cpp import Llama\n",
        "import gradio as gr\n",
        "import json\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# **********************************************************************\n",
        "# Param√®tres du mod√®le\n",
        "# **********************************************************************\n",
        "\n",
        "MODEL_PATH = \"Alisis.Q4.K.M.gguf\"\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=MODEL_PATH,\n",
        "    n_ctx=4096,\n",
        "    n_threads=6,\n",
        "    n_gpu_layers=50,\n",
        "    use_mlock=False,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "# **********************************************************************\n",
        "# Configuration par d√©faut\n",
        "# **********************************************************************\n",
        "\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"You are Alisis, a helpful, precise assistant created by the Alisis Research Team.\"\"\"\n",
        "SAVE_PATH = \"chat_history.json\"\n",
        "\n",
        "# **********************************************************************\n",
        "# Gestion de l'historique\n",
        "# **********************************************************************\n",
        "\n",
        "def save_chat(history):\n",
        "    os.makedirs(\"logs\", exist_ok=True)\n",
        "    filename = f\"logs/chat_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.json\"\n",
        "    with open(filename, \"w\", encoding='utf-8') as f:\n",
        "        json.dump(history, f, indent=2, ensure_ascii=False)\n",
        "    return filename\n",
        "\n",
        "def load_chat():\n",
        "    if os.path.exists(SAVE_PATH):\n",
        "        with open(SAVE_PATH, \"r\", encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    return []\n",
        "\n",
        "# **********************************************************************\n",
        "# Fonction de streaming\n",
        "# **********************************************************************\n",
        "\n",
        "def stream_response(message, history, system_prompt, temperature, max_tokens):\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "\n",
        "    # Ajouter l'historique\n",
        "    for human, ai in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": human})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": ai})\n",
        "\n",
        "    # Ajouter le message actuel\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    stream = llm.create_chat_completion(\n",
        "        messages=messages,\n",
        "        stream=True,\n",
        "        temperature=temperature,\n",
        "        top_p=0.9,\n",
        "        max_tokens=int(max_tokens),\n",
        "    )\n",
        "\n",
        "    partial = \"\"\n",
        "    for chunk in stream:\n",
        "        if \"choices\" in chunk and len(chunk[\"choices\"]) > 0:\n",
        "            delta = chunk[\"choices\"][0][\"delta\"]\n",
        "            if \"content\" in delta:\n",
        "                token = delta[\"content\"]\n",
        "                partial += token\n",
        "                yield partial\n",
        "\n",
        "# **********************************************************************\n",
        "# Interface utilisateur\n",
        "# **********************************************************************\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title='Alisis - Conversational AI') as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        <h1 style='text-align: center;'>Alisis - Research Assistant</h1>\n",
        "        <p style='text-align: center; color: gray;'>\n",
        "        Powered by Qwen2-7B (Quantized GGUF via llama.cpp)<br>\n",
        "        Created by the <b>Alisis Research Team</b>\n",
        "        </p>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            chatbot = gr.Chatbot(height=550, label=\"Conversation\")\n",
        "\n",
        "            with gr.Row():\n",
        "                msg = gr.Textbox(\n",
        "                    placeholder='üîç Tapez votre message ici...',\n",
        "                    show_label=False,\n",
        "                    lines=2,\n",
        "                )\n",
        "                send_btn = gr.Button(\"üöÄ Envoyer\", variant=\"primary\")\n",
        "\n",
        "            with gr.Row():\n",
        "                clear_btn = gr.Button(\"üóëÔ∏è Effacer la conversation\")\n",
        "                save_btn = gr.Button(\"üíæ Sauvegarder le chat\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"## ‚öôÔ∏è Param√®tres\")\n",
        "\n",
        "            system_prompt = gr.Textbox(\n",
        "                value=DEFAULT_SYSTEM_PROMPT,\n",
        "                label=\"Message syst√®me (r√¥le de l'assistant)\",\n",
        "                lines=4,\n",
        "            )\n",
        "\n",
        "            temperature = gr.Slider(\n",
        "                0.1, 1.5, value=0.7, step=0.05, label=\"Temp√©rature\"\n",
        "            )\n",
        "\n",
        "            max_tokens = gr.Slider(\n",
        "                128, 2048, value=512, step=64, label=\"Longueur maximale\"\n",
        "            )\n",
        "\n",
        "            download_link = gr.File(label=\"üì• T√©l√©charger le dernier historique\")\n",
        "\n",
        "    # **********************************************************************\n",
        "    # Logique interactive\n",
        "    # **********************************************************************\n",
        "\n",
        "    def user_message(message, chat_history):\n",
        "        return \"\", chat_history + [[message, None]]\n",
        "\n",
        "    def bot_response(chat_history, system_prompt, temperature, max_tokens):\n",
        "        message = chat_history[-1][0]\n",
        "        response = \"\"\n",
        "        for partial in stream_response(\n",
        "            message, chat_history[:-1], system_prompt, temperature, max_tokens\n",
        "        ):\n",
        "            response = partial\n",
        "            chat_history[-1][1] = response\n",
        "            yield chat_history\n",
        "\n",
        "    def clear_chat():\n",
        "        return []\n",
        "\n",
        "    def save_current(history):\n",
        "        filename = save_chat(history)\n",
        "        return gr.File(value=filename)\n",
        "\n",
        "    # **********************************************************************\n",
        "    # Liaison des actions\n",
        "    # **********************************************************************\n",
        "\n",
        "    msg.submit(\n",
        "        user_message,\n",
        "        [msg, chatbot],\n",
        "        [msg, chatbot],\n",
        "        queue=False\n",
        "    ).then(\n",
        "        bot_response,\n",
        "        [chatbot, system_prompt, temperature, max_tokens],\n",
        "        chatbot\n",
        "    )\n",
        "\n",
        "    send_btn.click(\n",
        "        user_message,\n",
        "        [msg, chatbot],\n",
        "        [msg, chatbot],\n",
        "        queue=False\n",
        "    ).then(\n",
        "        bot_response,\n",
        "        [chatbot, system_prompt, temperature, max_tokens],\n",
        "        chatbot\n",
        "    )\n",
        "\n",
        "    clear_btn.click(clear_chat, None, chatbot, queue=False)\n",
        "    save_btn.click(save_current, chatbot, download_link)\n",
        "\n",
        "    # **********************************************************************\n",
        "    # Lancement\n",
        "    # **********************************************************************\n",
        "\n",
        "    if __name__ == '__main__':\n",
        "        demo.queue()\n",
        "        demo.launch(server_name='0.0.0.0', share=True)"
      ],
      "metadata": {
        "id": "lprg1QQJolFZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}