{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZBRHewu5SDe96oNvowyXB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elidonda2-web/Alisia-7B-it-/blob/main/Chat_with_Alisia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Au cas o√π si il y pas un dossier nomm√© drive dans **Afficher l'explorateur des fichiers** clique sur c'est code:"
      ],
      "metadata": {
        "id": "LjtbVR8Y4SIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BuH98ESL4PGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bv0ivPiwndG-"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio\n",
        "!pip install -q llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Remplace la place de **votre idee** par le lien que vous avez copi√© dans Drive.\n",
        "**NB:** c'est utile seulement dans kaggle mais dans Drive c'est pas n√©cessaire voici le code:"
      ],
      "metadata": {
        "id": "-apFzlmixEhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown --quit\n",
        "!gdown Votre_id -O /kaggle/working/Alisia.Q4_K_M.gguf"
      ],
      "metadata": {
        "id": "2EFksJFAoG0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dans Drive:\n",
        "**MODEL_PATH=\"/content/drive/MyDrive/Q\n",
        "Alisia.Q4_K_M.gguf**\n",
        "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "##Mais dans kaggle:\n",
        "**MODEL_PATH=\"/kaggle/working/Alisia.Q4_K_M.gguf**"
      ],
      "metadata": {
        "id": "1gml3oVD20WM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **********************************************************************\n",
        "# Alisis - Chat UI complete (GGUF + Streaming) - VERSION CORRIG√âE\n",
        "# **********************************************************************\n",
        "from llama_cpp import Llama\n",
        "import gradio as gr\n",
        "import json\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# **********************************************************************\n",
        "# Param√®tres du mod√®le\n",
        "# **********************************************************************\n",
        "\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Alisia.Q4_K_M.gguf\"  # ‚úÖ tu vas remplacer quand du sera dans kaggle par /kaggle/working/Alisia.Q4_K_M.gguf\n",
        "\n",
        "# V√©rification du fichier mod√®le\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(f\"‚ùå Fichier mod√®le non trouv√©: {MODEL_PATH}\")\n",
        "    print(\"üìÅ Fichiers dans le r√©pertoire courant:\")\n",
        "    for file in os.listdir(\".\"):\n",
        "        if file.endswith(\".gguf\"):\n",
        "            print(f\"  - {file}\")\n",
        "    # Si le mod√®le n'existe pas, on cr√©e un simulateur pour tester l'interface\n",
        "    llm = None\n",
        "    print(\"‚ö†Ô∏è  Mode simulation activ√© (mod√®le non trouv√©)\")\n",
        "else:\n",
        "    llm = Llama(\n",
        "        model_path=MODEL_PATH,\n",
        "        n_ctx=4096,\n",
        "        n_threads=6,\n",
        "        n_gpu_layers=50,\n",
        "        use_mlock=False,\n",
        "        verbose=False,  # ‚úÖ verbose=False comme demand√©\n",
        "    )\n",
        "\n",
        "# **********************************************************************\n",
        "# Configuration par d√©faut\n",
        "# **********************************************************************\n",
        "\n",
        "DEFAULT_SYSTEM_PROMPT = \"You are Alisia, a helpful, precise assistant created by the Alisia Research Team.\"\n",
        "\n",
        "# **********************************************************************\n",
        "# Fonction de streaming CORRIG√âE\n",
        "# **********************************************************************\n",
        "\n",
        "def stream_response(message, history, system_prompt, temperature, max_tokens):\n",
        "    try:\n",
        "        print(f\"üîç D√©but g√©n√©ration - Message: {message[:50]}...\")\n",
        "\n",
        "        # Construction des messages dans le format correct\n",
        "        messages = []\n",
        "\n",
        "        # Message syst√®me\n",
        "        if system_prompt.strip():\n",
        "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "\n",
        "        # Historique de conversation\n",
        "        for human, ai in history:\n",
        "            if human and human.strip():\n",
        "                messages.append({\"role\": \"user\", \"content\": human})\n",
        "            if ai and ai.strip():\n",
        "                messages.append({\"role\": \"assistant\", \"content\": ai})\n",
        "\n",
        "        # Message actuel\n",
        "        messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "        print(f\"üì® Messages envoy√©s au mod√®le: {len(messages)}\")\n",
        "\n",
        "        # Simulation si mod√®le non charg√©\n",
        "        if llm is None:\n",
        "            response = f\"R√©ponse simul√©e pour: {message[:30]}...\"\n",
        "            for i in range(len(response)):\n",
        "                yield response[:i+1]\n",
        "            return\n",
        "\n",
        "        # Appel du mod√®le avec gestion d'erreur\n",
        "        stream = llm.create_chat_completion(\n",
        "            messages=messages,\n",
        "            stream=True,\n",
        "            temperature=float(temperature),\n",
        "            top_p=0.9,\n",
        "            max_tokens=int(max_tokens),\n",
        "            stop=[\"<|im_end|>\", \"</s>\", \"<|endoftext|>\"]\n",
        "        )\n",
        "\n",
        "        partial = \"\"\n",
        "        tokens_received = 0\n",
        "\n",
        "        for chunk in stream:\n",
        "            if \"choices\" in chunk and len(chunk[\"choices\"]) > 0:\n",
        "                choice = chunk[\"choices\"][0]\n",
        "                if \"delta\" in choice and \"content\" in choice[\"delta\"]:\n",
        "                    token = choice[\"delta\"][\"content\"]\n",
        "                    partial += token\n",
        "                    tokens_received += 1\n",
        "                    yield partial\n",
        "\n",
        "        print(f\"‚úÖ G√©n√©ration termin√©e - {tokens_received} tokens re√ßus\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"‚ùå Erreur lors de la g√©n√©ration: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        yield error_msg\n",
        "\n",
        "# **********************************************************************\n",
        "# Logique interactive CORRIG√âE\n",
        "# **********************************************************************\n",
        "\n",
        "def user_message(message, chat_history):\n",
        "    \"\"\"G√®re le message utilisateur\"\"\"\n",
        "    if not message or not message.strip():\n",
        "        return \"\", chat_history\n",
        "    return \"\", chat_history + [[message, None]]\n",
        "\n",
        "def bot_response(chat_history, system_prompt, temperature, max_tokens):\n",
        "    \"\"\"G√®re la r√©ponse du bot avec streaming\"\"\"\n",
        "    if not chat_history or not chat_history[-1][0]:\n",
        "        yield chat_history\n",
        "        return\n",
        "\n",
        "    message = chat_history[-1][0]\n",
        "    full_response = \"\"\n",
        "\n",
        "    try:\n",
        "        for partial in stream_response(\n",
        "            message,\n",
        "            chat_history[:-1] if chat_history else [],\n",
        "            system_prompt,\n",
        "            temperature,\n",
        "            max_tokens\n",
        "        ):\n",
        "            full_response = partial\n",
        "            # Met √† jour l'historique en temps r√©el\n",
        "            updated_history = chat_history[:-1] + [[message, full_response]]\n",
        "            yield updated_history\n",
        "\n",
        "        print(f\"üéâ R√©ponse finale: {full_response[:100]}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_response = f\"Erreur: {str(e)}\"\n",
        "        updated_history = chat_history[:-1] + [[message, error_response]]\n",
        "        yield updated_history\n",
        "\n",
        "def clear_chat():\n",
        "    \"\"\"Efface la conversation\"\"\"\n",
        "    return []\n",
        "\n",
        "def save_current(history):\n",
        "    \"\"\"Sauvegarde l'historique\"\"\"\n",
        "    try:\n",
        "        os.makedirs(\"logs\", exist_ok=True)\n",
        "        filename = f\"logs/chat_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.json\"\n",
        "        with open(filename, \"w\", encoding='utf-8') as f:\n",
        "            json.dump(history, f, indent=2, ensure_ascii=False)\n",
        "        return filename\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur sauvegarde: {e}\")\n",
        "        return None\n",
        "\n",
        "# **********************************************************************\n",
        "# Interface utilisateur\n",
        "# **********************************************************************\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title='Alisia - Conversational AI') as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        <div style='text-align: center;'>\n",
        "            <h1>Alisia - Research Assistant</h1>\n",
        "            <p style='color: #666;'>\n",
        "                Powered by Qwen2-7B (Quantized GGUF via llama.cpp)<br>\n",
        "                Created by the <b>Alisia Research Team</b>\n",
        "            </p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            chatbot = gr.Chatbot(\n",
        "                height=550,\n",
        "                label=\"Conversation\",\n",
        "                placeholder=\"La conversation appara√Ætra ici...\"\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                msg = gr.Textbox(\n",
        "                    placeholder='üí¨ Tapez votre message ici...',\n",
        "                    show_label=False,\n",
        "                    lines=2,\n",
        "                    container=False\n",
        "                )\n",
        "                send_btn = gr.Button(\"üöÄ Envoyer\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            with gr.Row():\n",
        "                clear_btn = gr.Button(\"üóëÔ∏è Effacer\", variant=\"secondary\")\n",
        "                save_btn = gr.Button(\"üíæ Sauvegarder\", variant=\"secondary\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### ‚öôÔ∏è Param√®tres\")\n",
        "\n",
        "            system_prompt = gr.Textbox(\n",
        "                value=DEFAULT_SYSTEM_PROMPT,\n",
        "                label=\"R√¥le du assistant\",\n",
        "                lines=3,\n",
        "            )\n",
        "\n",
        "            temperature = gr.Slider(\n",
        "                0.1, 2.0, value=0.7, step=0.1,\n",
        "                label=\"Temp√©rature (cr√©ativit√©)\"\n",
        "            )\n",
        "\n",
        "            max_tokens = gr.Slider(\n",
        "                128, 4096, value=512, step=64,\n",
        "                label=\"Tokens maximum\"\n",
        "            )\n",
        "\n",
        "            download_link = gr.File(\n",
        "                label=\"üì• T√©l√©charger l'historique\",\n",
        "                visible=False\n",
        "            )\n",
        "\n",
        "    # **********************************************************************\n",
        "    # Liaison des √©v√©nements\n",
        "    # **********************************************************************\n",
        "\n",
        "    # Message via Enter\n",
        "    msg.submit(\n",
        "        user_message,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[msg, chatbot],\n",
        "        queue=False\n",
        "    ).then(\n",
        "        bot_response,\n",
        "        inputs=[chatbot, system_prompt, temperature, max_tokens],\n",
        "        outputs=chatbot\n",
        "    )\n",
        "\n",
        "    # Message via bouton\n",
        "    send_btn.click(\n",
        "        user_message,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[msg, chatbot],\n",
        "        queue=False\n",
        "    ).then(\n",
        "        bot_response,\n",
        "        inputs=[chatbot, system_prompt, temperature, max_tokens],\n",
        "        outputs=chatbot\n",
        "    )\n",
        "\n",
        "    # Autres actions\n",
        "    clear_btn.click(\n",
        "        clear_chat,\n",
        "        outputs=chatbot\n",
        "    )\n",
        "\n",
        "    save_btn.click(\n",
        "        save_current,\n",
        "        inputs=chatbot,\n",
        "        outputs=download_link\n",
        "    )\n",
        "\n",
        "# **********************************************************************\n",
        "# Lancement CORRIG√â (sans concurrency_count)\n",
        "# **********************************************************************\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"üîß Diagnostic du mod√®le...\")\n",
        "    print(f\"üìÅ Mod√®le: {MODEL_PATH}\")\n",
        "\n",
        "    if llm is not None:\n",
        "        print(f\"üìä Contexte: {llm.n_ctx()} tokens\")\n",
        "        # Test rapide du mod√®le\n",
        "        try:\n",
        "            test_response = llm(\"Hello\", max_tokens=10, stop=[], echo=False)\n",
        "            print(\"‚úÖ Mod√®le fonctionne correctement\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur mod√®le: {e}\")\n",
        "    else:\n",
        "        print(\"üîÑ Mode simulation - interface seulement\")\n",
        "\n",
        "    print(\"üåê Lancement de l'interface...\")\n",
        "    demo.queue()  # ‚úÖ Ligne corrig√©e - sans concurrency_count\n",
        "    demo.launch(\n",
        "        server_name='0.0.0.0',\n",
        "        share=True,\n",
        "        show_error=True\n",
        "    )"
      ],
      "metadata": {
        "id": "wiV0lorM2uTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NB: *Tu dois toujours v√©rifier m√™me dans nos prochains Notebook: <**Afficher l'explorateur des fichiers**> si le dossier nomm√© drive est l√†*"
      ],
      "metadata": {
        "id": "wR1icw7s5U65"
      }
    }
  ]
}