{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/LG2dpsR0Q6B3T1HI7PDc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elidonda2-web/Alisia-7B-it-/blob/main/Alisia_7B_it.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4160db3a"
      },
      "source": [
        "# Task\n",
        "Install the dependencies, download and load the Alisia-7B-it model, and run inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a63b38d"
      },
      "source": [
        "## Install dependencies\n",
        "\n",
        "### Subtask:\n",
        "Install the necessary libraries for running the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c70276d1"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the required libraries using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96289cc5"
      },
      "source": [
        "%pip install -U transformers accelerate bitsandbytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52101057"
      },
      "source": [
        "## Download model\n",
        "\n",
        "### Subtask:\n",
        "Download the Alisia-7B-it model files.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell downloads and loads the Alisia-7B-it model and creates a text generation pipeline for inference.\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer\n",
        "import torch\n",
        "import threading\n",
        "\n",
        "model_name = \"Gems234/Alisia-7B-it\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_8bit=True,\n",
        "    dtype=None,\n",
        "    device_map=\"auto\",\n",
        "    llm_int8_enable_fp32_cpu_offload=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVrfNuZ-AXoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet uses the chatbot object, which was created in the previous cell by loading the \"Gems234/Alisia-7B-it\" model, to generate a response to the question \"What is the capital of France?\"."
      ],
      "metadata": {
        "id": "CrpjOggof_N0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chatbot(\"What is the capital of France?\", max_length=50)\n",
        "print(response[0]['generated_text'])"
      ],
      "metadata": {
        "id": "47Gb_LtvbtI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code demonstrates how to format prompts in the **Alpaca style**, which helps guide the model to follow instructions consistently.\n",
        "The prompt is structured into three parts:\n",
        "\n",
        "* **Instruction**: tells the model what role it should take or how it should respond (e.g., \"You are Alisia. Be concise and helpful.\").\n",
        "* **Input**: provides the actual user query or context (e.g., \"Where is the Eiffel Tower\").\n",
        "* **Response**: the model will generate a response corresponding to the question asked.\n",
        "\n",
        "By keeping this structure, the model can better understand and follow instructions.\n",
        "\n",
        "After preparing the prompt, the code tokenizes it and passes it to the model.\n",
        "The response is then generated with **real-time streaming**, meaning tokens are printed as soon as they are produced by the model.\n",
        "This provides a more interactive experience, similar to chatting with a live assistant."
      ],
      "metadata": {
        "id": "BHxZbOuEhfoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# Exemple\n",
        "instruction = \"You are Alisia. Be concise and helpful.\"\n",
        "input_text = \"where is the Eiffel Tower\"\n",
        "output_text = \"\"  # empty for the template to generate\n",
        "\n",
        "prompt = alpaca_prompt.format(instruction, input_text, output_text)\n",
        "\n",
        "# Tokenizer\n",
        "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Streaming setup\n",
        "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "generation_kwargs = dict(\n",
        "    **inputs,\n",
        "    max_new_tokens=50,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        "    eos_token_id=eos_token_id,\n",
        "    streamer=streamer,\n",
        ")\n",
        "\n",
        "# Start generation in parallel thread\n",
        "thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "thread.start()\n",
        "\n",
        "# Read tokens in real time\n",
        "for new_text in streamer:\n",
        "    print(new_text, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "Sd92ilwfkKH-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
